---
layout: post
title: Data Wrangling with MongoDB - Lesson7
tags:
- NoSQL
- Data Wrangling
published: true
---


## Data Quality
Getting the data in never enough, we need data to use so quality of data is very important all times. Checking the quality and making it better is called cleaning the data. So, what is data cleaning?

First thing to know about data cleaning is data cleaning is an iterative data. We iterate on detacting and then correcting the mistakes, missing things and so on... Value type of some data might be different than expected one, or in one field there are more than one values, they might not desing for our data type, or they might have some missing fields, and more. There are tons of causes which may cause the need for cleaning. We can generalize that if humans are involved of creating data, you may expect some problems in the data. Let's write the sources of dirty data in neat form:

- User Entry Errors
- Poorly Applied Coding Standarts
- Different Schemas
- Legacy Systems
- Evolving Applications
- Lack of Unique Identifiers
- Data Migration
- Programmer Error
- Corrupted by Transmission

Ok we know defined what is data cleaning and some causes of needs of data cleaning, but we did not answer the measurement of Data Quality. How do we measure the Data Quality:

- Validity: Conforms to a Schema
- Accuracy: Conforms to Gold Standard
	- Testing if the data is real or how accurate
- Completeness: All Records there?
- Consistency : Matches Other Data
- Uniformity : Same Units

What is the procedure of cleaning the data? In general,

### Audit The Data
First we check the data with programmatically. Meaning that checks the data according to uniformity. We can also run a statistical analysis to check the outliers.

### Data Cleaning Plan
Then we need to create a data cleaning plan, which includes identifying the causes of dirtiness, defining the operations to clean them, and testing the solution approaches. This might change depends on the situations

### Execute the Plan
In this phase of the procedure, we execute the plan that we defined one step ago.

### Manually Correct
If still has some errors or our programmatic solutions cannot correct the data, we do manual cleaning.

Let's apply this procedure in an example problem. The problem is in openstreet view map data that we want to solve using Python. We will be using [chicago map data](http://osm-extracted-metros.s3.amazonaws.com/chicago.osm.bz2). I am not going to put the data inside the GitHub because it is very large data set. There is code to handle this cleaning:

```Python
# These are the libraries that we will need
import xml.etree.cElementTree as ET
from collections import defaultdict
import re # Regular Expression module to parse the types of street names

# Opens the data
osm_file = open("chicago.osm", "r")

street_type_re = re.compile(r'\S+\.?$', re.IGNORECASE) #?
street_types = defaultdict(int)

# It gets street types like Ave, Avenue
def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()

        street_types[street_type] += 1

def print_sorted_dict(d):
    keys = d.keys()
    keys = sorted(keys, key=lambda s: s.lower())
    for k in keys:
        v = d[k]
        print "%s: %d" % (k, v)
# Checks if street name or not
def is_street_name(elem):
    return (elem.tag == "tag") and (elem.attrib['k'] == "addr:street")

# Gets the tags. and returs the how many time it occurs.
def audit():
    for event, elem in ET.iterparse(osm_file):
        if is_street_name(elem):
            audit_street_type(street_types, elem.attrib['v'])
    print_sorted_dict(street_types)

if __name__ == '__main__':
    audit()
```

When we run it we get different usage of same street type for example we have AVE, Ave, Ave., and Avenue for stating Avenue. Avenue was the most common one. So there is clearly a problem in uniformity.

- We need to set the validity constraints. We need to decide which constraints that we will use for the messy data, then the convert misspelled one to constraints. This is called _Auditing the Validity_. Constraints: Data type, range, regular expressions, foreing-key constraints, uniqueness.

-
